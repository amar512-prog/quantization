{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"raw","source":"#Deep Learning Model Compression Algorithm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"#!conda install -y gdown\n#!gdown --id 1feedmX0pDTltwE1uwmgIzIP7KFeNRkU9\n#!unzip '/content/gdrive/MyDrive/22_23_sem1/col870/*.zip'\n#!for z in '/kaggle/working/*.zip'; do unzip -q \"$z\"; done","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-12-15T12:19:17.231700Z","iopub.execute_input":"2022-12-15T12:19:17.232509Z","iopub.status.idle":"2022-12-15T12:20:41.102084Z","shell.execute_reply.started":"2022-12-15T12:19:17.232404Z","shell.execute_reply":"2022-12-15T12:20:41.100745Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting package metadata (current_repodata.json): done\nSolving environment: done\n\n\n==> WARNING: A newer version of conda exists. <==\n  current version: 22.9.0\n  latest version: 22.11.1\n\nPlease update conda by running\n\n    $ conda update -n base -c conda-forge conda\n\n\n\n## Package Plan ##\n\n  environment location: /opt/conda\n\n  added / updated specs:\n    - gdown\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    ca-certificates-2022.12.7  |       ha878542_0         143 KB  conda-forge\n    certifi-2022.12.7          |     pyhd8ed1ab_0         147 KB  conda-forge\n    filelock-3.8.2             |     pyhd8ed1ab_0          14 KB  conda-forge\n    gdown-4.6.0                |     pyhd8ed1ab_0          18 KB  conda-forge\n    openssl-1.1.1s             |       h0b41bf4_1         1.9 MB  conda-forge\n    ------------------------------------------------------------\n                                           Total:         2.2 MB\n\nThe following NEW packages will be INSTALLED:\n\n  filelock           conda-forge/noarch::filelock-3.8.2-pyhd8ed1ab_0 None\n  gdown              conda-forge/noarch::gdown-4.6.0-pyhd8ed1ab_0 None\n\nThe following packages will be UPDATED:\n\n  ca-certificates                      2022.9.24-ha878542_0 --> 2022.12.7-ha878542_0 None\n  certifi                            2022.9.24-pyhd8ed1ab_0 --> 2022.12.7-pyhd8ed1ab_0 None\n  openssl                                 1.1.1q-h166bdaf_0 --> 1.1.1s-h0b41bf4_1 None\n\n\n\nDownloading and Extracting Packages\nopenssl-1.1.1s       | 1.9 MB    | ##################################### | 100% \ncertifi-2022.12.7    | 147 KB    | ##################################### | 100% \ngdown-4.6.0          | 18 KB     | ##################################### | 100% \nfilelock-3.8.2       | 14 KB     | ##################################### | 100% \nca-certificates-2022 | 143 KB    | ##################################### | 100% \nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\nRetrieving notices: ...working... done\n/opt/conda/lib/python3.7/site-packages/gdown/cli.py:125: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  category=FutureWarning,\nDownloading...\nFrom: https://drive.google.com/uc?id=1feedmX0pDTltwE1uwmgIzIP7KFeNRkU9\nTo: /kaggle/working/4.zip\n100%|███████████████████████████████████████| 50.8M/50.8M [00:00<00:00, 110MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch, os\nimport torchvision\nimport torchvision.transforms as transforms\nfrom tqdm import tqdm\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n","metadata":{"execution":{"iopub.status.busy":"2022-12-15T12:30:12.087893Z","iopub.execute_input":"2022-12-15T12:30:12.088351Z","iopub.status.idle":"2022-12-15T12:30:14.308339Z","shell.execute_reply.started":"2022-12-15T12:30:12.088311Z","shell.execute_reply":"2022-12-15T12:30:14.307016Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_data_dir = '../input/data-images/4/train' # put path of training dataset\nval_data_dir = '../input/data-images/4/val' # put path of test dataset\ntest_data_dir = '../input/data-images/4/test' # put path of test dataset","metadata":{"execution":{"iopub.status.busy":"2022-12-15T12:29:35.962164Z","iopub.execute_input":"2022-12-15T12:29:35.962625Z","iopub.status.idle":"2022-12-15T12:29:35.968912Z","shell.execute_reply.started":"2022-12-15T12:29:35.962578Z","shell.execute_reply":"2022-12-15T12:29:35.967975Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"\n########################################################################\n# The output of torchvision datasets are PILImage images of range [0, 1].\n\n# Apply necessary image transfromations here \n\ntransform = transforms.Compose([transforms.ToTensor(),\n                                transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5, 0.5, 0.5])])\n\ntrainset = torchvision.datasets.ImageFolder(root= train_data_dir, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n                                          shuffle=True, num_workers=2)\n\nvalnset = torchvision.datasets.ImageFolder(root= val_data_dir, transform=transform)\nvalloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.ImageFolder(root= test_data_dir, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=4,\n                                         shuffle=False, num_workers=2)\n","metadata":{"execution":{"iopub.status.busy":"2022-12-15T12:30:17.181349Z","iopub.execute_input":"2022-12-15T12:30:17.182076Z","iopub.status.idle":"2022-12-15T12:30:19.041753Z","shell.execute_reply.started":"2022-12-15T12:30:17.182036Z","shell.execute_reply":"2022-12-15T12:30:19.040583Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"########################################################################\n# Define a Convolution Neural Network\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\n\n\nnum_epochs = 20         # desired number of training epochs.\nlearning_rate = 0.001  \ntest_num = 0 \n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n \n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(in_channels=64, out_channels=256, kernel_size=3, padding=1)\n        self.conv4 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n        \n        self.norm1 = nn.BatchNorm2d(64)\n        \n        self.pool = nn.MaxPool2d(kernel_size=3, stride=3)\n        self.fc3 = nn.Linear(in_features=256, out_features=33) # 5 is the number of classes here (for batch 3,4,5 out_features is 33)\n \n    def forward(self, x): \n \n        x = F.relu(self.conv1(x))\n        #x = self.pool(x)\n        \n        x = F.relu(self.conv2(x))\n        x = self.pool(x)\n        \n        x = F.relu(self.conv3(x))\n        #x = self.pool(x)\n        \n        x = F.relu(self.conv4(x))\n        x = self.pool(x)\n        \n        x = F.avg_pool2d(x, kernel_size=x.shape[2:])\n        x = x.view(x.shape[0], -1)\n        x = self.fc3(x)\n        \n        return x \n\n\n################### DO NOT EDIT THE BELOW CODE!!! #######################\n\nnet = Net()\n\n# transfer the model to GPU\nif torch.cuda.is_available():\n    net = net.cuda()\n\n########################################################################\n# Define a Loss function and optimizer\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n# Let's use a Classification Cross-Entropy loss and SGD with momentum.\n\nimport torch.optim as optim\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=3.4e-4)\n\nnum_params = np.sum([p.nelement() for p in net.parameters()])\nprint(num_params, ' parameters')","metadata":{"execution":{"iopub.status.busy":"2022-12-15T12:50:01.297034Z","iopub.execute_input":"2022-12-15T12:50:01.298016Z","iopub.status.idle":"2022-12-15T12:50:01.324660Z","shell.execute_reply.started":"2022-12-15T12:50:01.297971Z","shell.execute_reply":"2022-12-15T12:50:01.322804Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"785121  parameters\n","output_type":"stream"}]},{"cell_type":"code","source":"########################################################################\n# Train the network\n# ^^^^^^^^^^^^^^^^^^^^\n\ndef train(epoch, trainloader, optimizer, criterion):\n    running_loss = 0.0\n    for i, data in enumerate(tqdm(trainloader), 0):\n        # get the inputs\n        inputs, labels = data\n        if torch.cuda.is_available():\n            inputs, labels = inputs.cuda(), labels.cuda()\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n\n    print('epoch %d training loss: %.3f' %\n            (epoch + 1, running_loss / (len(trainloader))))\n    ","metadata":{"execution":{"iopub.status.busy":"2022-12-15T12:50:30.883339Z","iopub.execute_input":"2022-12-15T12:50:30.884873Z","iopub.status.idle":"2022-12-15T12:50:30.893354Z","shell.execute_reply.started":"2022-12-15T12:50:30.884809Z","shell.execute_reply":"2022-12-15T12:50:30.892237Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"train(epoch, trainloader, optimizer, criterion)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T12:50:40.301500Z","iopub.execute_input":"2022-12-15T12:50:40.301897Z","iopub.status.idle":"2022-12-15T12:52:13.775540Z","shell.execute_reply.started":"2022-12-15T12:50:40.301865Z","shell.execute_reply":"2022-12-15T12:52:13.773676Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":" 13%|█▎        | 417/3300 [01:33<10:45,  4.46it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_26/2592788434.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_26/2962440241.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, trainloader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"########################################################################\n# Let us look at how the network performs on the test dataset.\nl=[]\ndef test(testloader, model,s):\n    global test_num\n    test_num+=1\n    correct = 0\n    total = 0\n    l=[]\n    with torch.no_grad():\n        for data in tqdm(testloader):\n            images, labels = data\n            if torch.cuda.is_available():\n                images, labels = images.cuda(), labels.cuda()        \n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    if test_num%2==1:\n      print('Accuracy of the network on the %s images: %f %%' % (s,\n                                    100 * correct / total))\n    else:\n        print('Accuracy of the network on the %s images: %f %%' % (s,\n                                    100 * correct / total))\n        l.append(100 * correct / total)\n\n#########################################################################\n# get details of classes and class to index mapping in a directory\ndef find_classes(dir):\n    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n    classes.sort()\n    class_to_idx = {classes[i]: i for i in range(len(classes))}\n    return classes, class_to_idx\n\n\ndef classwise_test(testloader, model):\n########################################################################\n# class-wise accuracy\n\n    classes, _ = find_classes(train_data_dir)\n    n_class = len(classes) # number of classes\n\n    class_correct = list(0. for i in range(n_class))\n    class_total = list(0. for i in range(n_class))\n    with torch.no_grad():\n        for data in tqdm(testloader):\n            images, labels = data\n            if torch.cuda.is_available():\n                images, labels = images.cuda(), labels.cuda()        \n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            c = (predicted == labels).squeeze()\n            for i in range(4):\n                label = labels[i]\n                class_correct[label] += c[i].item()\n                class_total[label] += 1\n\n    for i in range(n_class):\n        print('Accuracy of %10s : %2f %%' % (\n            classes[i], 100 * class_correct[i] / class_total[i]))","metadata":{"execution":{"iopub.status.busy":"2022-12-15T12:32:33.418962Z","iopub.execute_input":"2022-12-15T12:32:33.419397Z","iopub.status.idle":"2022-12-15T12:32:33.436150Z","shell.execute_reply.started":"2022-12-15T12:32:33.419348Z","shell.execute_reply":"2022-12-15T12:32:33.434987Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#test base without quantization\ncheckpoint = torch.load('../input/model/bestmodel.pth',map_location=torch.device('cpu'))\nnet.load_state_dict(checkpoint)\ntest(trainloader,net,\"training\")\ntest(testloader, net,\"test\")\nclasswise_test(testloader, net)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T12:33:36.602104Z","iopub.execute_input":"2022-12-15T12:33:36.602551Z","iopub.status.idle":"2022-12-15T12:40:30.465563Z","shell.execute_reply.started":"2022-12-15T12:33:36.602514Z","shell.execute_reply":"2022-12-15T12:40:30.463979Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"100%|██████████| 3300/3300 [04:35<00:00, 11.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy of the network on the training images: 92.742424 %\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 825/825 [01:08<00:00, 12.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy of the network on the test images: 55.181818 %\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 825/825 [01:10<00:00, 11.74it/s]","output_type":"stream"},{"name":"stdout","text":"Accuracy of Ibizan_hound : 20.000000 %\nAccuracy of aircraft_carrier : 71.000000 %\nAccuracy of beer_bottle : 46.000000 %\nAccuracy of     bolete : 63.000000 %\nAccuracy of      boxer : 55.000000 %\nAccuracy of     carton : 32.000000 %\nAccuracy of       dome : 57.000000 %\nAccuracy of electric_guitar : 22.000000 %\nAccuracy of       file : 63.000000 %\nAccuracy of french_bulldog : 37.000000 %\nAccuracy of garbage_truck : 55.000000 %\nAccuracy of golden_retriever : 72.000000 %\nAccuracy of gordon_setter : 64.000000 %\nAccuracy of hair_slide : 59.000000 %\nAccuracy of  hourglass : 44.000000 %\nAccuracy of house_finch : 69.000000 %\nAccuracy of   komondor : 64.000000 %\nAccuracy of   malamute : 62.000000 %\nAccuracy of    meerkat : 65.000000 %\nAccuracy of pencil_box : 50.000000 %\nAccuracy of prayer_rug : 38.000000 %\nAccuracy of       reel : 31.000000 %\nAccuracy of rock_beauty : 78.000000 %\nAccuracy of scoreboard : 67.000000 %\nAccuracy of solar_dish : 46.000000 %\nAccuracy of      stage : 63.000000 %\nAccuracy of street_sign : 60.000000 %\nAccuracy of       tank : 73.000000 %\nAccuracy of  tile_roof : 54.000000 %\nAccuracy of tobacco_shop : 35.000000 %\nAccuracy of     trifle : 53.000000 %\nAccuracy of white_wolf : 64.000000 %\nAccuracy of       yawl : 89.000000 %\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"########################################################################\n# Quantizing the Convolution Neural Network\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nimport torch.quantization as qt\n\nnum_epochs = 20         # desired number of training epochs.\nlearning_rate = 0.001  \ntest_num = 0 \n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n         #QuantStub converts tensors from floating point to quantized\n        self.quant = qt.QuantStub()\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(in_channels=64, out_channels=256, kernel_size=3, padding=1)\n        self.conv4 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n\n        self.relu1 = nn.ReLU(inplace=True)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.relu3 = nn.ReLU(inplace=True)\n        self.relu4 = nn.ReLU(inplace=True)\n        self.norm1 = nn.BatchNorm2d(64)\n        \n        self.pool = nn.MaxPool2d(kernel_size=3, stride=3)\n        self.fc3 = nn.Linear(in_features=256, out_features=33) # 5 is the number of classes here (for batch 3,4,5 out_features is 33)\n        # DeQuantStub converts tensors from quantized to floating point\n        self.dequant = qt.DeQuantStub()\n \n    def forward(self, x): \n        # manually specify where tensors will be converted from floating\n        # point to quantized in the quantized model\n        x = self.quant(x)\n        x = self.relu1(self.conv1(x))\n        #x = self.pool(x)\n        \n        x = self.relu2(self.conv2(x))\n        x = self.pool(x)\n        \n        x = self.relu3(self.conv3(x))\n        x = self.pool(x)\n        \n        x = self.relu4(self.conv4(x))\n        #x = self.pool(x)\n        \n        x = F.avg_pool2d(x, kernel_size=x.shape[2:])\n        x = x.view(x.shape[0], -1)\n        x = self.fc3(x)\n        # manually specify where tensors will be converted from quantized\n        # to floating point in the quantized model\n        x = self.dequant(x)\n        return x \n\n\n################### DO NOT EDIT THE BELOW CODE!!! #######################\n\nnet = Net()\n\n# transfer the model to GPU\nif torch.cuda.is_available():\n    net = net.cuda()\n\n########################################################################\n# Define a Loss function and optimizer\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n# Let's use a Classification Cross-Entropy loss and SGD with momentum.\n\nimport torch.optim as optim\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=3.4e-4)\n\nnum_params = np.sum([p.nelement() for p in net.parameters()])\nprint(num_params, ' parameters')\n","metadata":{"execution":{"iopub.status.busy":"2022-12-15T12:46:37.513468Z","iopub.execute_input":"2022-12-15T12:46:37.513978Z","iopub.status.idle":"2022-12-15T12:46:37.542000Z","shell.execute_reply.started":"2022-12-15T12:46:37.513936Z","shell.execute_reply":"2022-12-15T12:46:37.540740Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"785121  parameters\n","output_type":"stream"}]},{"cell_type":"code","source":"#test dynamic quantization\ncheckpoint = torch.load('../input/model/bestmodel.pth',map_location=torch.device('cpu'))\nnet.load_state_dict(checkpoint)\nmodel_net8 = torch.quantization.quantize_dynamic(net, {torch.nn.Linear}, dtype=torch.qint8)\nmodel_net8.to(torch.device('cpu'))\nmodel_net8.eval()\ntest(trainloader,model_net8,\"train\")\ntest(testloader, model_net8,\"test\")\nclasswise_test(testloader, model_net8)\nos.makedirs('./models', exist_ok=True)\ntorch.save(model_net8.state_dict(), './models/bestmodel_d.pth')","metadata":{"execution":{"iopub.status.busy":"2022-12-15T12:52:23.200576Z","iopub.execute_input":"2022-12-15T12:52:23.201783Z","iopub.status.idle":"2022-12-15T12:58:28.486703Z","shell.execute_reply.started":"2022-12-15T12:52:23.201743Z","shell.execute_reply":"2022-12-15T12:58:28.485321Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"100%|██████████| 3300/3300 [04:05<00:00, 13.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy of the network on the train images: 92.651515 %\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 825/825 [00:59<00:00, 13.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy of the network on the test images: 55.212121 %\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 825/825 [00:59<00:00, 13.85it/s]","output_type":"stream"},{"name":"stdout","text":"Accuracy of Ibizan_hound : 20.000000 %\nAccuracy of aircraft_carrier : 72.000000 %\nAccuracy of beer_bottle : 47.000000 %\nAccuracy of     bolete : 64.000000 %\nAccuracy of      boxer : 56.000000 %\nAccuracy of     carton : 32.000000 %\nAccuracy of       dome : 58.000000 %\nAccuracy of electric_guitar : 22.000000 %\nAccuracy of       file : 62.000000 %\nAccuracy of french_bulldog : 39.000000 %\nAccuracy of garbage_truck : 53.000000 %\nAccuracy of golden_retriever : 73.000000 %\nAccuracy of gordon_setter : 65.000000 %\nAccuracy of hair_slide : 59.000000 %\nAccuracy of  hourglass : 43.000000 %\nAccuracy of house_finch : 69.000000 %\nAccuracy of   komondor : 64.000000 %\nAccuracy of   malamute : 62.000000 %\nAccuracy of    meerkat : 65.000000 %\nAccuracy of pencil_box : 49.000000 %\nAccuracy of prayer_rug : 37.000000 %\nAccuracy of       reel : 31.000000 %\nAccuracy of rock_beauty : 78.000000 %\nAccuracy of scoreboard : 66.000000 %\nAccuracy of solar_dish : 46.000000 %\nAccuracy of      stage : 63.000000 %\nAccuracy of street_sign : 60.000000 %\nAccuracy of       tank : 74.000000 %\nAccuracy of  tile_roof : 54.000000 %\nAccuracy of tobacco_shop : 34.000000 %\nAccuracy of     trifle : 53.000000 %\nAccuracy of white_wolf : 63.000000 %\nAccuracy of       yawl : 89.000000 %\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"#test static quantization\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nimport torch.quantization as qt\n\nnum_epochs = 20         # desired number of training epochs.\nlearning_rate = 0.001  \ntest_num = 0 \n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n         #QuantStub converts tensors from floating point to quantized\n        self.quant = qt.QuantStub()\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=64, out_channels=256, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n       \n        self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n        self.relu1 = nn.ReLU(inplace=False)\n        self.relu2 = nn.ReLU(inplace=False)\n        self.relu3 = nn.ReLU(inplace=False)\n        self.relu4 = nn.ReLU(inplace=False)\n        self.norm1 = nn.BatchNorm2d(64)\n        \n        self.pool = nn.MaxPool2d(kernel_size=3, stride=3)\n        self.fc3 = nn.Linear(in_features=256, out_features=33) # 5 is the number of classes here (for batch 3,4,5 out_features is 33)\n        # DeQuantStub converts tensors from quantized to floating point\n        self.dequant = qt.DeQuantStub()\n \n    def forward(self, x): \n        # manually specify where tensors will be converted from floating\n        # point to quantized in the quantized model\n        x = self.quant(x)\n        x = self.relu1(self.conv1(x))\n        #x = self.pool(x)\n        x = self.relu4(self.conv4(x))\n        \n        x = self.pool(x)\n        \n        x = self.relu2(self.conv2(x))\n        \n        #x = self.pool(x)\n        x = self.relu3(self.conv3(x))\n        \n        x = self.pool(x)\n        \n        x = F.avg_pool2d(x, kernel_size=x.shape[2:])\n        x = x.view(x.shape[0], -1)\n        x = self.fc3(x)\n        # manually specify where tensors will be converted from quantized\n        # to floating point in the quantized model\n        x = self.dequant(x)\n        return x \n\n\n################### DO NOT EDIT THE BELOW CODE!!! #######################\n\nnet = Net()\n\n# transfer the model to GPU\nif torch.cuda.is_available():\n    net = net.cuda()\n\n########################################################################\n# Define a Loss function and optimizer\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n# Let's use a Classification Cross-Entropy loss and SGD with momentum.\n\nimport torch.optim as optim\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=3.4e-4)\n\nnum_params = np.sum([p.nelement() for p in net.parameters()])\nprint(num_params, ' parameters')\n\nmodel_fp32 = net\nmodel_fp32.eval()\ncheckpoint = torch.load('../input/model/bestmodel.pth',map_location=torch.device('cpu'))\nmodel_fp32.load_state_dict(checkpoint)\nmodel_fp32.eval()\nprint(model_fp32.conv1)\nmodel_fp32.qconfig = torch.quantization.get_default_qconfig('fbgemm')\nmodel_fp32_fused = torch.quantization.fuse_modules(\n    model_fp32, [['conv1', 'relu1'],['conv4', 'relu4'],['conv2', 'relu2'],['conv3', 'relu3'],]\n)\nmodel_fp32_prepared = torch.quantization.prepare(model_fp32_fused)\n\n# quantization algorithm calibration happens here\nprint('Start Calibration')\nos.makedirs('./models', exist_ok=True)\n\n\nfor epoch in range(num_epochs):  # loop over the dataset multiple times\n    print('epoch ', epoch + 1)\n    train_quant(epoch, trainloader, optimizer, criterion,model_fp32_prepared)\n    #test(valloader, model_fp32_prepared)\n    #classwise_test(valloader, net)\n    test(testloader, model_fp32_prepared)\n    # save model checkpoint \n    torch.save(model_fp32_prepared.state_dict(), './models/model'+str(epoch)+'.pth')      \n\nprint('performing test')\n#test(testloader, net)\nclasswise_test(testloader, net)\nprint(\"max accuracy:\",np.max(l))\nprint('Finished Calibration')\n\nmodel_fp32_prepared.eval()\nmodel_int8 = torch.quantization.convert(model_fp32_prepared, inplace=True)\nprint(model_int8.conv1)\n\ntest(trainloader,model_int8)\ntest(testloader, model_int8)\nclasswise_test(testloader, model_int8)\nos.makedirs('./models', exist_ok=True)\ntorch.save(model_net8.state_dict(), './models/bestmodel_s.pth')","metadata":{"execution":{"iopub.status.busy":"2022-12-15T12:52:16.465053Z","iopub.execute_input":"2022-12-15T12:52:16.465488Z","iopub.status.idle":"2022-12-15T12:52:16.547796Z","shell.execute_reply.started":"2022-12-15T12:52:16.465454Z","shell.execute_reply":"2022-12-15T12:52:16.541621Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"785121  parameters\nConv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\nStart Calibration\nepoch  1\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_26/21234639.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# loop over the dataset multiple times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mtrain_quant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_fp32_prepared\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0;31m#test(valloader, model_fp32_prepared)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;31m#classwise_test(valloader, net)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_quant' is not defined"],"ename":"NameError","evalue":"name 'train_quant' is not defined","output_type":"error"}]},{"cell_type":"code","source":"from torch.quantization.observer import MinMaxObserver, MovingAverageMinMaxObserver, HistogramObserver\ntorch.save(model_fp32_prepared.state_dict(), './models/model_sc.pth')\nmodel_fp32_prepared.eval()\nmodel_int8 = torch.quantization.convert(model_fp32_prepared, inplace=True)\nprint(model_int8.conv1)\n\ntest(trainloader,model_int8)\ntest(testloader, model_int8)\nclasswise_test(testloader, model_int8)\nos.makedirs('./models', exist_ok=True)\ntorch.save(model_net8.state_dict(), './models/bestmodel_sc1.pth')","metadata":{"execution":{"iopub.status.busy":"2022-12-14T06:41:28.499044Z","iopub.status.idle":"2022-12-14T06:41:28.499638Z","shell.execute_reply.started":"2022-12-14T06:41:28.499305Z","shell.execute_reply":"2022-12-14T06:41:28.499333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test static quantization\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nimport torch.quantization as qt\nfrom torch.quantization.observer import MinMaxObserver, MovingAverageMinMaxObserver, HistogramObserver\n\n\nnum_epochs = 20        # desired number of training epochs.\nlearning_rate = 0.001  \ntest_num = 0 \n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n         #QuantStub converts tensors from floating point to quantized\n        self.quant = qt.QuantStub()\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=64, out_channels=256, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n       \n        self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n        self.relu1 = nn.ReLU(inplace=False)\n        self.relu2 = nn.ReLU(inplace=False)\n        self.relu3 = nn.ReLU(inplace=False)\n        self.relu4 = nn.ReLU(inplace=False)\n        self.norm1 = nn.BatchNorm2d(64)\n        \n        self.pool = nn.MaxPool2d(kernel_size=3, stride=3)\n        self.fc3 = nn.Linear(in_features=256, out_features=33) # 5 is the number of classes here (for batch 3,4,5 out_features is 33)\n        # DeQuantStub converts tensors from quantized to floating point\n        self.dequant = qt.DeQuantStub()\n \n    def forward(self, x): \n        # manually specify where tensors will be converted from floating\n        # point to quantized in the quantized model\n        x = self.quant(x)\n        x = self.relu1(self.conv1(x))\n        #x = self.pool(x)\n        x = self.relu4(self.conv4(x))\n        \n        x = self.pool(x)\n        \n        x = self.relu2(self.conv2(x))\n        \n        #x = self.pool(x)\n        x = self.relu3(self.conv3(x))\n        \n        x = self.pool(x)\n        \n        x = F.avg_pool2d(x, kernel_size=x.shape[2:])\n        x = x.view(x.shape[0], -1)\n        x = self.fc3(x)\n        # manually specify where tensors will be converted from quantized\n        # to floating point in the quantized model\n        x = self.dequant(x)\n        return x \n\n\n################### DO NOT EDIT THE BELOW CODE!!! #######################\n\nnet = Net()\n\n# transfer the model to GPU\nif torch.cuda.is_available():\n    net = net.cuda()\n\n########################################################################\n# Define a Loss function and optimizer\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n# Let's use a Classification Cross-Entropy loss and SGD with momentum.\n\nimport torch.optim as optim\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=3.4e-4)\n\nnum_params = np.sum([p.nelement() for p in net.parameters()])\nprint(num_params, ' parameters')\n\nmodel_fp32 = net\ncheckpoint = torch.load('../input/model/bestmodel.pth',map_location=torch.device('cpu'))\nmodel_fp32.load_state_dict(checkpoint)\nmodel_fp32.train()\nprint(model_fp32.conv1)\nmodel_fp32.qconfig = torch.quantization.get_default_qconfig('fbgemm')\nmodel_fp32_fused = torch.quantization.fuse_modules(\n    model_fp32, [['conv1', 'relu1'],['conv4', 'relu4'],['conv2', 'relu2'],['conv3', 'relu3'],]\n)\nmodel_fp32_prepared = torch.quantization.prepare_qat(model_fp32_fused)\n\n# quantization algorithm calibration happens here\nprint('Start Calibration')\nos.makedirs('./models', exist_ok=True)\n\n\nfor epoch in range(num_epochs):  # loop over the dataset multiple times\n    print('epoch ', epoch + 1)\n    train_quant(epoch, trainloader, optimizer, criterion,model_fp32_prepared)\n    #test(valloader, model_fp32_prepared)\n    #classwise_test(valloader, net)\n    test(testloader, model_fp32_prepared)\n    # save model checkpoint \n    torch.save(model_fp32_prepared.state_dict(), './models/modelq'+str(epoch)+'.pth')      \n\nprint('performing test')\nprint('Finished Calibration')\nmodel_int8 = torch.quantization.convert(model_fp32_prepared, inplace=True)\nprint(model_int8.conv1.weight())\n#res = model_int8(input_fp32)\n#checkpoint = torch.load('../input/model/bestmodel.pth',map_location=torch.device('cpu'))\n#model_int8.load_state_dict(checkpoint)\n#model_fp32 = torch.quantization.quantize_dynamic(net, {torch.nn.Linear}, dtype=torch.qint8)\n#model_fp32.to(torch.device('cpu'))\n#model_fp32.eval()\ntest(trainloader,model_int8)\ntest(testloader, model_int8)\nclasswise_test(testloader, model_int8)\nos.makedirs('./models', exist_ok=True)\ntorch.save(model_net8.state_dict(), './models/bestmodel_q.pth')","metadata":{"execution":{"iopub.status.busy":"2022-12-15T12:42:27.976535Z","iopub.execute_input":"2022-12-15T12:42:27.976965Z","iopub.status.idle":"2022-12-15T12:42:28.070549Z","shell.execute_reply.started":"2022-12-15T12:42:27.976933Z","shell.execute_reply":"2022-12-15T12:42:28.068897Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"785121  parameters\nConv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\nStart Calibration\nepoch  1\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/ao/quantization/observer.py:179: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n  reduce_range will be deprecated in a future release of PyTorch.\"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_26/2990477038.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# loop over the dataset multiple times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mtrain_quant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_fp32_prepared\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;31m#test(valloader, model_fp32_prepared)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;31m#classwise_test(valloader, net)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_quant' is not defined"],"ename":"NameError","evalue":"name 'train_quant' is not defined","output_type":"error"}]},{"cell_type":"code","source":"model_int8 = torch.quantization.convert(model_fp32_prepared, inplace=True)\nprint(model_int8.conv1)\n#res = model_int8(input_fp32)\n#checkpoint = torch.load('../input/model/bestmodel.pth',map_location=torch.device('cpu'))\n#model_int8.load_state_dict(checkpoint)\n#model_fp32 = torch.quantization.quantize_dynamic(net, {torch.nn.Linear}, dtype=torch.qint8)\n#model_fp32.to(torch.device('cpu'))\n#model_fp32.eval()\ntest(trainloader,model_int8)\ntest(testloader, model_int8)\nclasswise_test(testloader, model_int8)\nos.makedirs('./models', exist_ok=True)\ntorch.save(model_net8.state_dict(), './models/bestmodel_q.pth')","metadata":{"execution":{"iopub.status.busy":"2022-12-14T06:41:28.503241Z","iopub.status.idle":"2022-12-14T06:41:28.503772Z","shell.execute_reply.started":"2022-12-14T06:41:28.503503Z","shell.execute_reply":"2022-12-14T06:41:28.503529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model_fp32_prepared.state_dict(), './models/bestmodel_q2.pth')","metadata":{"execution":{"iopub.status.busy":"2022-12-14T06:41:28.504877Z","iopub.status.idle":"2022-12-14T06:41:28.505242Z","shell.execute_reply.started":"2022-12-14T06:41:28.505051Z","shell.execute_reply":"2022-12-14T06:41:28.505087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########################################################################\n# Train the network\n# ^^^^^^^^^^^^^^^^^^^^\n\ndef train_quant(epoch, trainloader, optimizer, criterion, net_model):\n    running_loss = 0.0\n    for i, data in enumerate(tqdm(trainloader), 0):\n        # get the inputs\n        inputs, labels = data\n        if torch.cuda.is_available():\n            inputs, labels = inputs.cuda(), labels.cuda()\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net_model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n\n    print('epoch %d training loss: %.3f' %\n            (epoch + 1, running_loss / (len(trainloader))))\n    ","metadata":{"execution":{"iopub.status.busy":"2022-12-14T06:41:28.506471Z","iopub.status.idle":"2022-12-14T06:41:28.506848Z","shell.execute_reply.started":"2022-12-14T06:41:28.506661Z","shell.execute_reply":"2022-12-14T06:41:28.506679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test QAT\n########################################################################\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nimport torch.quantization as qt\n\nnum_epochs = 20         # desired number of training epochs.\nlearning_rate = 0.001  \ntest_num = 0 \n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n         #QuantStub converts tensors from floating point to quantized\n        self.quant = qt.QuantStub()\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=64, out_channels=256, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n       \n        self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.relu3 = nn.ReLU(inplace=True)\n        self.relu4 = nn.ReLU(inplace=True)\n        self.norm1 = nn.BatchNorm2d(64)\n        \n        self.pool = nn.MaxPool2d(kernel_size=3, stride=3)\n        self.fc3 = nn.Linear(in_features=256, out_features=33) # 5 is the number of classes here (for batch 3,4,5 out_features is 33)\n        # DeQuantStub converts tensors from quantized to floating point\n        self.dequant = qt.DeQuantStub()\n \n    def forward(self, x): \n        # manually specify where tensors will be converted from floating\n        # point to quantized in the quantized model\n        x = self.quant(x)\n        x = self.conv1(x)\n        x = self.relu1(x)\n        #x = self.pool(x)\n        x = self.conv4(x)\n        x = self.relu4(x)\n        \n        x = self.pool(x)\n        x = self.conv2(x)\n        x = self.relu2(x)\n        \n        #x = self.pool(x)\n        x = self.conv3(x)\n        x = self.relu3(x)\n        \n        x = self.pool(x)\n        \n        x = F.avg_pool2d(x, kernel_size=x.shape[2:])\n        x = x.view(x.shape[0], -1)\n        x = self.fc3(x)\n        # manually specify where tensors will be converted from quantized\n        # to floating point in the quantized model\n        x = self.dequant(x)\n        return x \n\n\n\nnet = Net()\n\n# transfer the model to GPU\nif torch.cuda.is_available():\n    net = net.cuda()\n\n########################################################################\n# Define a Loss function and optimizer\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n# Let's use a Classification Cross-Entropy loss and SGD with momentum.\n\nimport torch.optim as optim\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=3.4e-4)\n\nnum_params = np.sum([p.nelement() for p in net.parameters()])\nprint(num_params, ' parameters')\n\nmodel_fp32 = net\nmodel_fp32.train()\nmodel_fp32.qconfig = torch.quantization.get_default_qconfig('fbgemm')\nmodel_fp32_fused = torch.quantization.fuse_modules(\n    model_fp32, [['conv1', 'relu1'],['conv4', 'relu4'],['conv2', 'relu2'],['conv3', 'relu3'],]\n)\nmodel_fp32_prepared = torch.quantization.prepare_qat(model_fp32_fused)\n\n# quantization algorithm calibration happens here\n\nprint('Start Training')\nos.makedirs('./models', exist_ok=True)\n\n\nfor epoch in range(num_epochs):  # loop over the dataset multiple times\n    print('epoch ', epoch + 1)\n    train_quant(epoch, trainloader, optimizer, criterion,model_fp32_prepared)\n    #test(valloader, model_fp32_prepared)\n    #classwise_test(valloader, net)\n    test(testloader, model_fp32_prepared)\n    # save model checkpoint \n    torch.save(net.state_dict(), './models/model'+str(epoch)+'.pth')      \n\nprint('performing test')\n#test(testloader, net)\nclasswise_test(testloader, net)\nprint(\"max accuracy:\",np.max(l))\nprint('Finished Training')\nmodel_fp32_prepared.eval()\nmodel_int8 = torch.quantization.convert(model_fp32_prepared)\n#res = model_int8(input_fp32)\ncheckpoint = torch.load('../input/model/bestmodel.pth',map_location=torch.device('cpu'))\nnet.load_state_dict(checkpoint)\n\ntest(trainloader,model_int8)\ntest(testloader, model_int8)\nclasswise_test(testloader, model_int8)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T06:41:28.508556Z","iopub.status.idle":"2022-12-14T06:41:28.509003Z","shell.execute_reply.started":"2022-12-14T06:41:28.508776Z","shell.execute_reply":"2022-12-14T06:41:28.508794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########################################################################\n# Define a Convolution Neural Network\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\n\n\n\nnum_epochs = 20         # desired number of training epochs.\nlearning_rate = 0.001  \ntest_num = 0 \n\nclass UNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.q_1 = torch.quantization.QuantStub()\n        self.conv_1_1 = nn.Conv2d(3, 64, 3)\n        torch.nn.init.kaiming_normal_(self.conv_1_1.weight)\n        self.relu_1_2 = nn.ReLU()\n        self.norm_1_3 = nn.BatchNorm2d(64)\n        self.conv_1_4 = nn.Conv2d(64, 64, 3)\n        torch.nn.init.kaiming_normal_(self.conv_1_4.weight)\n        self.relu_1_5 = nn.ReLU()\n        self.norm_1_6 = nn.BatchNorm2d(64)\n        self.pool_1_7 = nn.MaxPool2d(2)\n        \n        self.conv_2_1 = nn.Conv2d(64, 128, 3)\n        torch.nn.init.kaiming_normal_(self.conv_2_1.weight)        \n        self.relu_2_2 = nn.ReLU()\n        self.norm_2_3 = nn.BatchNorm2d(128)\n        self.conv_2_4 = nn.Conv2d(128, 128, 3)\n        torch.nn.init.kaiming_normal_(self.conv_2_4.weight)        \n        self.relu_2_5 = nn.ReLU()\n        self.norm_2_6 = nn.BatchNorm2d(128)\n        self.pool_2_7 = nn.MaxPool2d(2)\n        \n        self.conv_3_1 = nn.Conv2d(128, 256, 3)\n        torch.nn.init.kaiming_normal_(self.conv_3_1.weight)\n        self.relu_3_2 = nn.ReLU()\n        self.norm_3_3 = nn.BatchNorm2d(256)\n        self.conv_3_4 = nn.Conv2d(256, 256, 3)\n        torch.nn.init.kaiming_normal_(self.conv_3_4.weight)\n        self.relu_3_5 = nn.ReLU()\n        self.norm_3_6 = nn.BatchNorm2d(256)\n        self.pool_3_7 = nn.MaxPool2d(2)\n        \n        self.conv_4_1 = nn.Conv2d(256, 512, 3)\n        torch.nn.init.kaiming_normal_(self.conv_4_1.weight)\n        self.relu_4_2 = nn.ReLU()\n        self.norm_4_3 = nn.BatchNorm2d(512)\n        self.conv_4_4 = nn.Conv2d(512, 512, 3)\n        torch.nn.init.kaiming_normal_(self.conv_4_4.weight)\n        self.relu_4_5 = nn.ReLU()\n        self.norm_4_6 = nn.BatchNorm2d(512)\n        self.dq_1 = torch.quantization.DeQuantStub()\n        \n        # deconv is the '2D transposed convolution operator'\n        self.deconv_5_1 = nn.ConvTranspose2d(512, 256, (2, 2), 2)\n        # 61x61 -> 48x48 crop\n        self.c_crop_5_2 = lambda x: x[:, :, 6:54, 6:54]\n        self.concat_5_3 = lambda x, y: torch.cat((x, y), dim=1)\n        self.q_2 = torch.quantization.QuantStub()\n        self.conv_5_4 = nn.Conv2d(512, 256, 3)\n        torch.nn.init.kaiming_normal_(self.conv_5_4.weight)        \n        self.relu_5_5 = nn.ReLU()\n        self.norm_5_6 = nn.BatchNorm2d(256)\n        self.conv_5_7 = nn.Conv2d(256, 256, 3)\n        torch.nn.init.kaiming_normal_(self.conv_5_7.weight)\n        self.relu_5_8 = nn.ReLU()\n        self.norm_5_9 = nn.BatchNorm2d(256)\n        self.dq_2 = torch.quantization.DeQuantStub()\n        \n        self.deconv_6_1 = nn.ConvTranspose2d(256, 128, (2, 2), 2)\n        # 121x121 -> 88x88 crop\n        self.c_crop_6_2 = lambda x: x[:, :, 17:105, 17:105]\n        self.concat_6_3 = lambda x, y: torch.cat((x, y), dim=1)\n        self.q_3 = torch.quantization.QuantStub()\n        self.conv_6_4 = nn.Conv2d(256, 128, 3)\n        torch.nn.init.kaiming_normal_(self.conv_6_4.weight)\n        self.relu_6_5 = nn.ReLU()\n        self.norm_6_6 = nn.BatchNorm2d(128)\n        self.conv_6_7 = nn.Conv2d(128, 128, 3)\n        torch.nn.init.kaiming_normal_(self.conv_6_7.weight)\n        self.relu_6_8 = nn.ReLU()\n        self.norm_6_9 = nn.BatchNorm2d(128)\n        self.dq_3 = torch.quantization.DeQuantStub()\n        \n        self.deconv_7_1 = nn.ConvTranspose2d(128, 64, (2, 2), 2)\n        # 252x252 -> 168x168 crop\n        self.c_crop_7_2 = lambda x: x[:, :, 44:212, 44:212]\n        self.concat_7_3 = lambda x, y: torch.cat((x, y), dim=1)\n        self.q_4 = torch.quantization.QuantStub()\n        self.conv_7_4 = nn.Conv2d(128, 64, 3)\n        torch.nn.init.kaiming_normal_(self.conv_7_4.weight)\n        self.relu_7_5 = nn.ReLU()\n        self.norm_7_6 = nn.BatchNorm2d(64)\n        self.conv_7_7 = nn.Conv2d(64, 64, 3)\n        torch.nn.init.kaiming_normal_(self.conv_7_7.weight)        \n        self.relu_7_8 = nn.ReLU()\n        self.norm_7_9 = nn.BatchNorm2d(64)\n        \n        # 1x1 conv ~= fc; n_classes = 33\n        self.conv_8_1 = nn.Conv2d(64, 33, 1)\n        self.fcout = nn.Linear(in_features=33, out_features=33)\n        self.dq_4 = torch.quantization.DeQuantStub()\n        \n        # residual connections need to be dequantized seperately\n        self.dq_resid_1 = torch.quantization.DeQuantStub()\n        self.dq_resid_2 = torch.quantization.DeQuantStub()\n        self.dq_resid_3 = torch.quantization.DeQuantStub()\n        \n\n    def forward(self, x):\n        x = self.q_1(x)        \n        x = self.conv_1_1(x)\n        x = self.relu_1_2(x)\n        x = self.norm_1_3(x)\n        x = self.conv_1_4(x)\n        x = self.relu_1_5(x)\n        x_resid_1_quantized = self.norm_1_6(x)\n        x = self.pool_1_7(x_resid_1_quantized)\n        x_resid_1 = self.dq_resid_1(x_resid_1_quantized)\n        \n        x = self.conv_2_1(x)\n        x = self.relu_2_2(x)\n        x = self.norm_2_3(x)\n        x = self.conv_2_4(x)\n        x = self.relu_2_5(x)\n        x_resid_2_quantized = self.norm_2_6(x)\n        x = self.pool_2_7(x_resid_2_quantized)\n        x_resid_2 = self.dq_resid_2(x_resid_2_quantized)\n        \n        x = self.conv_3_1(x)\n        x = self.relu_3_2(x)\n        x = self.norm_3_3(x)\n        x = self.conv_3_4(x)\n        x = self.relu_3_5(x)\n        x_resid_3_quantized = self.norm_3_6(x)\n        x = self.pool_3_7(x_resid_3_quantized)\n        x_resid_3 = self.dq_resid_3(x_resid_3_quantized)\n        \n        x = self.conv_4_1(x)\n        x = self.relu_4_2(x)\n        x = self.norm_4_3(x)        \n        x = self.conv_4_4(x)\n        x = self.relu_4_5(x)\n        x = self.norm_4_6(x)\n        x = self.dq_1(x)\n        \n        x = self.deconv_5_1(x)\n        x = self.concat_5_3(self.c_crop_5_2(x_resid_3), x)\n        x = self.q_2(x)\n        x = self.conv_5_4(x)\n        x = self.relu_5_5(x)\n        x = self.norm_5_6(x)\n        x = self.conv_5_7(x)\n        x = self.relu_5_8(x)\n        x = self.norm_5_9(x)\n        x = self.dq_2(x)\n        \n        x = self.deconv_6_1(x)\n        x = self.concat_6_3(self.c_crop_6_2(x_resid_2), x)\n        x = self.q_3(x)\n        x = self.conv_6_4(x)\n        x = self.relu_6_5(x)\n        x = self.norm_6_6(x)\n        x = self.conv_6_7(x)\n        x = self.relu_6_8(x)\n        x = self.norm_6_9(x)\n        x = self.dq_3(x)\n        \n        x = self.deconv_7_1(x)\n        x = self.concat_7_3(self.c_crop_7_2(x_resid_1), x)\n        x = self.q_4(x)\n        x = self.conv_7_4(x)\n        x = self.relu_7_5(x)\n        x = self.norm_7_6(x)\n        x = self.conv_7_7(x)\n        x = self.relu_7_8(x)\n        x = self.norm_7_9(x)\n        \n        x = self.conv_8_1(x)\n        x = F.avg_pool2d(x, kernel_size=x.shape[2:])\n        x = x.view(x.shape[0], -1)\n        x = self.fcout(x)\n        x = self.dq_4(x)        \n        return x\n","metadata":{"execution":{"iopub.status.busy":"2022-12-14T06:41:28.510710Z","iopub.status.idle":"2022-12-14T06:41:28.512587Z","shell.execute_reply.started":"2022-12-14T06:41:28.512389Z","shell.execute_reply":"2022-12-14T06:41:28.512410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net = UNet()\n\n# transfer the model to GPU\nif torch.cuda.is_available():\n    net = net.cuda()\n\n########################################################################\n# Define a Loss function and optimizer\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n# Let's use a Classification Cross-Entropy loss and SGD with momentum.\n\nimport torch.optim as optim\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=3.4e-4)\n\nnum_params = np.sum([p.nelement() for p in net.parameters()])\nprint(num_params, ' parameters')\ncheckpoint = torch.load('../input/models/model18.pth',map_location=torch.device('cpu'))\nnet.load_state_dict(checkpoint)\nnet.eval()\nmodel_fp32 = net\nmodel_fp32.qconfig = torch.quantization.get_default_qconfig('fbgemm')\nmodel_fp32_fused = torch.quantization.fuse_modules(\n    model_fp32,\n        [\n            ['conv_1_1', 'relu_1_2'],\n            ['conv_1_4', 'relu_1_5'],\n            ['conv_2_1', 'relu_2_2'],\n            ['conv_2_4', 'relu_2_5'],\n            ['conv_3_1', 'relu_3_2'],\n            ['conv_3_4', 'relu_3_5'],\n            ['conv_4_1', 'relu_4_2'],\n            ['conv_4_4', 'relu_4_5'],\n        ]\n    )\n\nmodel_fp32.eval()\nmodel_fp32_prepared = torch.quantization.prepare(model_fp32_fused)\n\n# quantization algorithm calibration happens here\n# this example uses just a single sample, but obvious in prod you will\n# want to use some meaningful subset of your training or test set\n# instead.\n#input_fp32 = torch.randn(64, 3, 3, 3)\n#print(input_fp32)\n#model_fp32_prepared(input_fp32.cuda())\nmodel_fp32_prepared.eval()\nmodel_int8 = torch.quantization.convert(model_fp32_prepared)\n#res = model_int8(input_fp32)\n\n#model_fp32 = torch.quantization.quantize_dynamic(net, {torch.nn.Linear}, dtype=torch.qint8)\n#model_fp32.to(torch.device('cpu'))\n#model_fp32.eval()\ntest(trainloader,model_int8)\ntest(testloader, model_int8)\nclasswise_test(testloader, model_int8)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T06:41:28.513441Z","iopub.status.idle":"2022-12-14T06:41:28.514228Z","shell.execute_reply.started":"2022-12-14T06:41:28.513916Z","shell.execute_reply":"2022-12-14T06:41:28.513944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n########################################################################\n# The output of torchvision datasets are PILImage images of range [0, 1].\n\n# Apply necessary image transfromations here \n\ntransform = transforms.Compose([ transforms.Resize((164, 164)), transforms.ToTensor(),\n                                transforms.Pad(46, padding_mode='reflect'),\n                                transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5, 0.5, 0.5])])\n\n\ntrain_data_dir = '/kaggle/working/4/train' # put path of training dataset\nval_data_dir = '/kaggle/working/4/val' # put path of test dataset\ntest_data_dir = '/kaggle/working/4/test' # put path of test dataset\n\ntrainset = torchvision.datasets.ImageFolder(root= train_data_dir, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n                                          shuffle=True, num_workers=2)\n\nvalnset = torchvision.datasets.ImageFolder(root= val_data_dir, transform=transform)\nvalloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.ImageFolder(root= test_data_dir, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=4,\n                                         shuffle=False, num_workers=2)\n","metadata":{"execution":{"iopub.status.busy":"2022-12-14T06:41:28.515628Z","iopub.status.idle":"2022-12-14T06:41:28.516157Z","shell.execute_reply.started":"2022-12-14T06:41:28.515872Z","shell.execute_reply":"2022-12-14T06:41:28.515897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########################################################################\n# Train the network\n# ^^^^^^^^^^^^^^^^^^^^\n\ndef train(epoch, trainloader, optimizer, criterion):\n    running_loss = 0.0\n    for i, data in enumerate(tqdm(trainloader), 0):\n        # get the inputs\n        inputs, labels = data\n        if torch.cuda.is_available():\n            inputs, labels = inputs.cuda(), labels.cuda()\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        #print(outputs.size(),labels.size())\n        #print(labels)\n        #print(outputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n\n    print('epoch %d training loss: %.3f' %\n            (epoch + 1, running_loss / (len(trainloader))))\n    ","metadata":{"execution":{"iopub.status.busy":"2022-12-14T06:41:28.517980Z","iopub.status.idle":"2022-12-14T06:41:28.518778Z","shell.execute_reply.started":"2022-12-14T06:41:28.518474Z","shell.execute_reply":"2022-12-14T06:41:28.518502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########################################################################\n# Let us look at how the network performs on the test dataset.\n\nl=[]\ndef test(testloader, model):\n    global test_num\n    test_num+=1\n    correct = 0\n    total = 0\n    l=[]\n    with torch.no_grad():\n        for data in tqdm(testloader):\n            images, labels = data\n            if torch.cuda.is_available():\n                images, labels = images.cuda(), labels.cuda()        \n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    if test_num%2==1:\n      print('Accuracy of the network on the valid images: %f %%' % (\n                                    100 * correct / total))\n    else:\n        print('Accuracy of the network on the test images: %f %%' % (\n                                    100 * correct / total))\n        l.append(100 * correct / total)\n\n#########################################################################\n# get details of classes and class to index mapping in a directory\ndef find_classes(dir):\n    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n    classes.sort()\n    class_to_idx = {classes[i]: i for i in range(len(classes))}\n    return classes, class_to_idx\n\n\ndef classwise_test(testloader, model):\n########################################################################\n# class-wise accuracy\n\n    classes, _ = find_classes(train_data_dir)\n    n_class = len(classes) # number of classes\n\n    class_correct = list(0. for i in range(n_class))\n    class_total = list(0. for i in range(n_class))\n    with torch.no_grad():\n        for data in tqdm(testloader):\n            images, labels = data\n            if torch.cuda.is_available():\n                images, labels = images.cuda(), labels.cuda()        \n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            c = (predicted == labels).squeeze()\n            for i in range(4):\n                label = labels[i]\n                class_correct[label] += c[i].item()\n                class_total[label] += 1\n\n    for i in range(n_class):\n        print('Accuracy of %10s : %2f %%' % (\n            classes[i], 100 * class_correct[i] / class_total[i]))","metadata":{"execution":{"iopub.status.busy":"2022-12-14T06:41:28.520400Z","iopub.status.idle":"2022-12-14T06:41:28.520934Z","shell.execute_reply.started":"2022-12-14T06:41:28.520666Z","shell.execute_reply":"2022-12-14T06:41:28.520691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\n\nnum_epochs = 20         # desired number of training epochs.\nlearning_rate = 0.001  \ntest_num = 0 \nclass UNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_1_1 = nn.Conv2d(3, 64, 3)\n        torch.nn.init.kaiming_normal_(self.conv_1_1.weight)\n        self.relu_1_2 = nn.ReLU()\n        self.norm_1_3 = nn.BatchNorm2d(64)\n        self.conv_1_4 = nn.Conv2d(64, 64, 3)\n        torch.nn.init.kaiming_normal_(self.conv_1_4.weight)\n        self.relu_1_5 = nn.ReLU()\n        self.norm_1_6 = nn.BatchNorm2d(64)\n        self.pool_1_7 = nn.MaxPool2d(2)\n        \n        self.conv_2_1 = nn.Conv2d(64, 128, 3)\n        torch.nn.init.kaiming_normal_(self.conv_2_1.weight)        \n        self.relu_2_2 = nn.ReLU()\n        self.norm_2_3 = nn.BatchNorm2d(128)\n        self.conv_2_4 = nn.Conv2d(128, 128, 3)\n        torch.nn.init.kaiming_normal_(self.conv_2_4.weight)        \n        self.relu_2_5 = nn.ReLU()\n        self.norm_2_6 = nn.BatchNorm2d(128)\n        self.pool_2_7 = nn.MaxPool2d(2)\n        \n        self.conv_3_1 = nn.Conv2d(128, 256, 3)\n        torch.nn.init.kaiming_normal_(self.conv_3_1.weight)\n        self.relu_3_2 = nn.ReLU()\n        self.norm_3_3 = nn.BatchNorm2d(256)\n        self.conv_3_4 = nn.Conv2d(256, 256, 3)\n        torch.nn.init.kaiming_normal_(self.conv_3_4.weight)\n        self.relu_3_5 = nn.ReLU()\n        self.norm_3_6 = nn.BatchNorm2d(256)\n        self.pool_3_7 = nn.MaxPool2d(2)\n        \n        self.conv_4_1 = nn.Conv2d(256, 512, 3)\n        torch.nn.init.kaiming_normal_(self.conv_4_1.weight)\n        self.relu_4_2 = nn.ReLU()\n        self.norm_4_3 = nn.BatchNorm2d(512)\n        self.conv_4_4 = nn.Conv2d(512, 512, 3)\n        torch.nn.init.kaiming_normal_(self.conv_4_4.weight)\n        self.relu_4_5 = nn.ReLU()\n        self.norm_4_6 = nn.BatchNorm2d(512)\n        \n        # deconv is the '2D transposed convolution operator'\n        self.deconv_5_1 = nn.ConvTranspose2d(512, 256, (2, 2), 2)\n        # 61x61 -> 48x48 crop\n        self.c_crop_5_2 = lambda x: x[:, :, 6:54, 6:54]\n        self.concat_5_3 = lambda x, y: torch.cat((x, y), dim=1)\n        self.conv_5_4 = nn.Conv2d(512, 256, 3)\n        torch.nn.init.kaiming_normal_(self.conv_5_4.weight)        \n        self.relu_5_5 = nn.ReLU()\n        self.norm_5_6 = nn.BatchNorm2d(256)\n        self.conv_5_7 = nn.Conv2d(256, 256, 3)\n        torch.nn.init.kaiming_normal_(self.conv_5_7.weight)\n        self.relu_5_8 = nn.ReLU()\n        self.norm_5_9 = nn.BatchNorm2d(256)\n        \n        self.deconv_6_1 = nn.ConvTranspose2d(256, 128, (2, 2), 2)\n        # 121x121 -> 88x88 crop\n        self.c_crop_6_2 = lambda x: x[:, :, 17:105, 17:105]\n        self.concat_6_3 = lambda x, y: torch.cat((x, y), dim=1)\n        self.conv_6_4 = nn.Conv2d(256, 128, 3)\n        torch.nn.init.kaiming_normal_(self.conv_6_4.weight)\n        self.relu_6_5 = nn.ReLU()\n        self.norm_6_6 = nn.BatchNorm2d(128)\n        self.conv_6_7 = nn.Conv2d(128, 128, 3)\n        torch.nn.init.kaiming_normal_(self.conv_6_7.weight)\n        self.relu_6_8 = nn.ReLU()\n        self.norm_6_9 = nn.BatchNorm2d(128)\n        \n        self.deconv_7_1 = nn.ConvTranspose2d(128, 64, (2, 2), 2)\n        # 252x252 -> 168x168 crop\n        self.c_crop_7_2 = lambda x: x[:, :, 44:212, 44:212]\n        self.concat_7_3 = lambda x, y: torch.cat((x, y), dim=1)\n        self.conv_7_4 = nn.Conv2d(128, 64, 3)\n        torch.nn.init.kaiming_normal_(self.conv_7_4.weight)\n        self.relu_7_5 = nn.ReLU()\n        self.norm_7_6 = nn.BatchNorm2d(64)\n        self.conv_7_7 = nn.Conv2d(64, 64, 3)\n        torch.nn.init.kaiming_normal_(self.conv_7_7.weight)        \n        self.relu_7_8 = nn.ReLU()\n        self.norm_7_9 = nn.BatchNorm2d(64)\n        \n        # 1x1 conv ~= fc; n_classes = 33\n        self.conv_8_1 = nn.Conv2d(64, 33, 1)\n        self.fcout = nn.Linear(in_features=33, out_features=33)\n\n    def forward(self, x):\n        x = self.conv_1_1(x)\n        x = self.relu_1_2(x)\n        x = self.norm_1_3(x)\n        x = self.conv_1_4(x)\n        x = self.relu_1_5(x)\n        x_residual_1 = self.norm_1_6(x)\n        x = self.pool_1_7(x_residual_1)\n        \n        x = self.conv_2_1(x)\n        x = self.relu_2_2(x)\n        x = self.norm_2_3(x)\n        x = self.conv_2_4(x)\n        x = self.relu_2_5(x)\n        x_residual_2 = self.norm_2_6(x)\n        x = self.pool_2_7(x_residual_2)\n        \n        x = self.conv_3_1(x)\n        x = self.relu_3_2(x)\n        x = self.norm_3_3(x)\n        x = self.conv_3_4(x)\n        x = self.relu_3_5(x)\n        x_residual_3 = self.norm_3_6(x)\n        x = self.pool_3_7(x_residual_3)\n        \n        x = self.conv_4_1(x)\n        x = self.relu_4_2(x)\n        x = self.norm_4_3(x)        \n        x = self.conv_4_4(x)\n        x = self.relu_4_5(x)\n        x = self.norm_4_6(x)\n        \n        x = self.deconv_5_1(x)\n        x = self.concat_5_3(self.c_crop_5_2(x_residual_3), x)\n        x = self.conv_5_4(x)\n        x = self.relu_5_5(x)\n        x = self.norm_5_6(x)\n        x = self.conv_5_7(x)\n        x = self.relu_5_8(x)\n        x = self.norm_5_9(x)\n        \n        x = self.deconv_6_1(x)\n        x = self.concat_6_3(self.c_crop_6_2(x_residual_2), x)\n        x = self.conv_6_4(x)\n        x = self.relu_6_5(x)\n        x = self.norm_6_6(x)\n        x = self.conv_6_7(x)\n        x = self.relu_6_8(x)\n        x = self.norm_6_9(x)\n        \n        x = self.deconv_7_1(x)\n        x = self.concat_7_3(self.c_crop_7_2(x_residual_1), x)\n        x = self.conv_7_4(x)\n        x = self.relu_7_5(x)\n        x = self.norm_7_6(x)\n        x = self.conv_7_7(x)\n        x = self.relu_7_8(x)\n        x = self.norm_7_9(x)\n        \n        x = self.conv_8_1(x)\n        x = F.avg_pool2d(x, kernel_size=x.shape[2:])\n        x = x.view(x.shape[0], -1)\n        x = self.fcout(x)\n        #x = self.fcout(x)\n        \n        return x\nnet = UNet()\n\n# transfer the model to GPU\nif torch.cuda.is_available():\n    net = net.cuda()\n\n########################################################################\n# Define a Loss function and optimizer\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n# Let's use a Classification Cross-Entropy loss and SGD with momentum.\n\nimport torch.optim as optim\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=3.4e-4)\n\nnum_params = np.sum([p.nelement() for p in net.parameters()])\nprint(num_params, ' parameters')\nprint('Start Training')\nos.makedirs('./models', exist_ok=True)\n\n\nfor epoch in range(num_epochs):  # loop over the dataset multiple times\n    print('epoch ', epoch + 1)\n    train(epoch, trainloader, optimizer, criterion)\n    test(valloader, net)\n    #classwise_test(valloader, net)\n    test(testloader, net)\n    # save model checkpoint \n    torch.save(net.state_dict(), './models/model'+str(epoch)+'.pth')      \n\nprint('performing test')\n#test(testloader, net)\nclasswise_test(testloader, net)\nprint(\"max accuracy:\",np.max(l))\nprint('Finished Training')\n\n# Saving our trained model\ntorch.save(net.state_dict(), './models/bestmodel.pth')\nprint(net)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T06:41:28.522941Z","iopub.status.idle":"2022-12-14T06:41:28.523510Z","shell.execute_reply.started":"2022-12-14T06:41:28.523212Z","shell.execute_reply":"2022-12-14T06:41:28.523238Z"},"trusted":true},"execution_count":null,"outputs":[]}]}